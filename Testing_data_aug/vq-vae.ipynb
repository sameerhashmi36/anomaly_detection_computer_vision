{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100CustomDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        # Load the CIFAR-100 dataset\n",
    "        self.data = datasets.CIFAR100(root=root, train=train, download=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get an image and its label\n",
    "        img, label = self.data[idx]\n",
    "\n",
    "        # Apply the transformations if provided\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Resize to 64x64 for VQ-VAE input\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR100CustomDataset(root='./data', train=True, transform=transform)\n",
    "test_dataset = CIFAR100CustomDataset(root='./data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_single_image(image, label):\n",
    "    plt.imshow(image.permute(1, 2, 0) * 0.5 + 0.5)  # Unnormalize and permute to HWC\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_single_image(images[10], labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VQVAE(nn.Module):\n",
    "#     def __init__(self, num_embeddings, embedding_dim):\n",
    "#         super(VQVAE, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # [64, 64, 32, 32]\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # [64, 128, 16, 16]\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # [64, 256, 8, 8]\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, embedding_dim, kernel_size=1)  # Final Layer: [batch_size, embedding_dim, 8, 8]\n",
    "#         )\n",
    "        \n",
    "#         self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "#          # Decoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(embedding_dim, 256, kernel_size=4, stride=2, padding=1),  # [64, 256, 16, 16]\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # [64, 128, 32, 32]\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # [64, 64, 64, 64]\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 3, kernel_size=1)  # Output Layer: [batch_size, 3, 64, 64]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder output\n",
    "#         z_e = self.encoder(x)  # Shape: [batch_size, embedding_dim, 8, 8]\n",
    "#         print(f'z_e shape: {z_e.shape}')\n",
    "\n",
    "#         # Flatten the encoder output\n",
    "#         z_e_flattened = z_e.view(z_e.size(0), z_e.size(1), -1)  # Shape: [batch_size, embedding_dim, 64]\n",
    "#         print(f'z_e_flattened shape: {z_e_flattened.shape}')\n",
    "\n",
    "#         # Quantization\n",
    "#         distances = (z_e_flattened.unsqueeze(2) - self.embeddings.weight.unsqueeze(0)).pow(2).sum(1)\n",
    "#         encoding_indices = torch.argmin(distances, dim=2)\n",
    "\n",
    "#         z_q = self.embeddings(encoding_indices)  # Shape: [batch_size, height * width, embedding_dim]\n",
    "#         z_q = z_q.permute(0, 2, 1).view(z_e.size(0), z_e.size(1), z_e.size(2)*z_e.size(2), z_e.size(3))  # Shape: [batch_size, embedding_dim, 8, 8]\n",
    "#         print(f'z_q shape after reshaping: {z_q.shape}')\n",
    "\n",
    "#         # Decoder\n",
    "#         reconstructed = self.decoder(z_q)  # Shape: [batch_size, 3, 64, 64]\n",
    "#         print(f'reconstructed shape: {reconstructed.shape}')\n",
    "\n",
    "#         return z_e, z_q, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Upsample from 8x8 to 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Upsample from 16x16 to 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),   # Upsample from 32x32 to 64x64\n",
    "            nn.Sigmoid()  # Output in range [0, 1] for images\n",
    "        )\n",
    "\n",
    "    def forward(self, z_q):\n",
    "        # Reshape z_q to match the input size expected by the decoder\n",
    "        # z_q will be of shape [batch_size, 64, 64] before reshaping\n",
    "        z_q = z_q.view(z_q.size(0), 64, 8, 8)  # Reshape back to 8x8 before feeding to the decoder\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        reconstructed = self.decoder_layers(z_q)\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # Output: [32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Output: [64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)   # Output: [64, 8, 8]\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        # Decoder using the Decoder class\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder output\n",
    "        z_e = self.encoder(x)  # Shape: [batch_size, 64, 8, 8]\n",
    "        print(f'z_e shape: {z_e.shape}')\n",
    "        \n",
    "        # Flatten the encoder output\n",
    "        z_e_flattened = z_e.view(z_e.size(0), z_e.size(1), -1)  # Shape: [batch_size, 64, 64]\n",
    "        print(f'z_e_flattened shape: {z_e_flattened.shape}')\n",
    "\n",
    "        # Reshape embeddings to compare with z_e_flattened for distances calculation\n",
    "        # z_e_flattened: [batch_size, embedding_dim, height*width] -> [batch_size, embedding_dim, flattened size]\n",
    "        # embeddings.weight: [num_embeddings, embedding_dim] -> [1, num_embeddings, embedding_dim]\n",
    "        z_e_flattened = z_e_flattened.permute(0, 2, 1)  # Shape: [batch_size, 64, embedding_dim]\n",
    "        \n",
    "        distances = (z_e_flattened.unsqueeze(2) - self.embeddings.weight.unsqueeze(0)).pow(2).sum(3)  # Shape: [batch_size, 64, num_embeddings]\n",
    "        encoding_indices = torch.argmin(distances, dim=2)  # Find closest embedding\n",
    "        print(f'distances shape: {distances.shape}')\n",
    "        print(f'encoding_indices shape: {encoding_indices.shape}')\n",
    "\n",
    "        # Retrieve the quantized latent vectors (z_q) from the embeddings\n",
    "        z_q = self.embeddings(encoding_indices)\n",
    "        print(f'z_q shape: {z_q.shape}')  # Shape: [batch_size, height * width, embedding_dim]\n",
    "\n",
    "        # Reshape to match encoder dimensions for decoder\n",
    "        z_q = z_q.permute(0, 2, 1)  # Shape: [batch_size, 64, 512]\n",
    "        z_q = z_q.view(z_e.size(0), z_e.size(1), z_e.size(2), z_e.size(3))  # Reshaped for decoder input\n",
    "        \n",
    "        print(f'z_q shape after reshaping: {z_q.shape}')  # Output: [batch_size, 64, 8, 8]\n",
    "\n",
    "        # Decoder\n",
    "        reconstructed = self.decoder(z_q)  # Should match input image size\n",
    "        print(f'reconstructed shape: {reconstructed.shape}')  # Output: [batch_size, 3, 64, 64]\n",
    "\n",
    "        return z_e, z_q, reconstructed  # Return encoded, quantized, and reconstructed representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(num_embeddings=512, embedding_dim=64)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (embeddings): Embedding(512, 64)\n",
       "  (decoder): Decoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (VQ-VAE):   0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_e shape: torch.Size([64, 64, 4, 4])\n",
      "z_e_flattened shape: torch.Size([64, 64, 16])\n",
      "distances shape: torch.Size([64, 16, 512])\n",
      "encoding_indices shape: torch.Size([64, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (VQ-VAE):   0%|          | 0/782 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_q shape: torch.Size([64, 16, 64])\n",
      "z_q shape after reshaping: torch.Size([64, 64, 4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 64, 8, 8]' is invalid for input of size 65536",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m z_e, z_q, reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get encoded, quantized, and reconstructed outputs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 1. Reconstruction loss (MSE loss between input images and reconstructed images)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(reconstructed, images) \n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 74\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_q shape after reshaping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz_q\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output: [batch_size, 64, 8, 8]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_q\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Should match input image size\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstructed shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Output: [batch_size, 3, 64, 64]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_e, z_q, reconstructed\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z_q)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z_q):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Reshape z_q to match the input size expected by the decoder\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# z_q will be of shape [batch_size, 64, 64] before reshaping\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     z_q \u001b[38;5;241m=\u001b[39m \u001b[43mz_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape back to 8x8 before feeding to the decoder\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Pass through decoder layers\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers(z_q)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 64, 8, 8]' is invalid for input of size 65536"
     ]
    }
   ],
   "source": [
    "# List to store loss values for plotting\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Variable initialization to store the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()  # Model in training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # TQDM progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/50 (VQ-VAE)\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        z_e, z_q, reconstructed = model(images)  # Get encoded, quantized, and reconstructed outputs\n",
    "        \n",
    "        # Calculate loss\n",
    "        # 1. Reconstruction loss (MSE loss between input images and reconstructed images)\n",
    "        reconstruction_loss = F.mse_loss(reconstructed, images) \n",
    "        \n",
    "        # 2. Commitment loss (VQ loss)\n",
    "        commitment_loss = F.mse_loss(z_e.detach(), z_q) + F.mse_loss(z_e, z_q.detach())\n",
    "\n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + commitment_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update TQDM progress bar with the current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Calculate the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_values.append(avg_loss)\n",
    "    \n",
    "    # Print the average loss for the current epoch\n",
    "    print(f\"Epoch [{epoch+1}/50], Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # --- Validation loop ---\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations during validation\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            val_z_e, val_z_q, val_reconstructed = model(val_images)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_reconstruction_loss = F.mse_loss(val_reconstructed, val_images) \n",
    "            val_commitment_loss = F.mse_loss(val_z_e.detach(), val_z_q) + F.mse_loss(val_z_e, val_z_q.detach())\n",
    "            val_loss = val_reconstruction_loss + val_commitment_loss\n",
    "            \n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    # Calculate the average validation loss for this epoch\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_loss_values.append(avg_val_loss)\n",
    "\n",
    "    # Print the average validation loss for the current epoch\n",
    "    print(f\"Epoch [{epoch+1}/50], Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # --- Model selection based on validation loss ---\n",
    "    if avg_val_loss < best_val_loss and (epoch+1) % 5 == 0:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(f\"Validation loss improved. Saving the best model at epoch {epoch+1}.\")\n",
    "        torch.save(model.state_dict(), f'model_vqvae_cifar100/best_model_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
